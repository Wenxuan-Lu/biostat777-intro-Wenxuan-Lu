[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Wenxuan Lu",
    "section": "",
    "text": "I am a first-year PhD student in Biostatistics at Johns Hopkins University. My research interest lies in statistical genetics and disease risk prediction. Besides school, I love travelling and figure skating."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a first-year PhD student in Biostatistics at Johns Hopkins University under the advisement of Dr. Nilanjan Chatterjee. Prior to this, I obtained a bachelor’s degree in Applied Mathematics and Statistics with a secondary major in Computer Science at Johns Hopkins University. My research interest lies in statistical genetics, and I had a summer intern on multi-ancestry polygenic risk scores. The internship brought up my attention to the health equity issues and the statistical problem of data integration. During my PhD journey, I would like to develop scalable statistical methods and computational tools to further investigate these problems."
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Example analysis",
    "section": "",
    "text": "In natural language processing, topic models detect abstract ‘topics’ underlying a collection of documents. As research papers increase in number and accessibility, the problem of article categorization has become a hot topic. Moreover, different categories can overlap with each other, posing new problems of characterizing documents as a mixture of topics.\nIn this analysis, I would like to explore the ability of latent Dirichlet allocation (LDA), one of the most popular topic models, in detecting the underlying topics of research papers (Blei, Ng, and Jordan 2003). Unlike hard clustering algorithms, LDA treats each article as a mixture of multiple topics and aims to recover the mixture proportion. How accurate can the papers be clustered? What are the key features distinguishing different clusters?\n\n\nA hard clustering algorithm assigns every item to one and only one cluster. LDA is a type of soft clustering, where every item can belong to multiple clusters.\nThe intended audience are researchers who want to find an efficient way to find papers of interests. Moreover, the results can give insights on the strengths and weaknesses of LDA for people who want to use the model in other settings."
  },
  {
    "objectID": "analysis.knit.html",
    "href": "analysis.knit.html",
    "title": "Example analysis",
    "section": "",
    "text": "Introduction\nIn natural language processing, a topic model is a statistical model that detects abstract ‘topics’ among a collection of documents. As research papers increase in number and accessibility, the problem of article categorization has attracted many attention. In this analysis, I would like to explore the ability of Latent Dirichlet Allocation (LDA), one of the most popular topic models, in detecting the underlying topics of research papers. This can provide researchers with an efficient way to filter papers of interests. Moreover, the results can give an insight about the strengths and weaknesses of LDA for people who want to use the model in other settings.\n\n\nDataset\n\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(topicmodels)\nlibrary(quanteda)\n\nThe Kaggle dataset Topic Modeling for Research Articles contains the title and abstract of 20972 scientific papers published between 2017 and 2020. They are from 6 topics: Computer Science, Physics, Mathematics, Statistics, Quantitative Biology, and Quantitative Finance. The data dictionary recreated with information from the Kaggle website is shown below.\n\n\n\n\n\nField Name\nData Type\nDescription\n\n\n\n\nID\nint\nUnique ID\n\n\nTITLE\nchr\nTitle of the Paper\n\n\nABSTRACT\nchr\nAbstract Summary of the Paper\n\n\nComputer.Science\nint\n0/1, Whether Under Computer Science Topic\n\n\nPhysics\nint\n0/1, Whether Under Physics Topic\n\n\nMathematics\nint\n0/1, Whether Under Mathematics Topic\n\n\nStatistics\nint\n0/1, Whether Under Statistics Topic\n\n\nQuantitative.Biology\nint\n0/1, Whether Under Quantitative Biology Topic\n\n\nQuantitative.Finance\nint\n0/1, Whether Under Quantitative Finance Topic\n\n\n\n\n\n\n\n\n\nPreprocessing\nSince LDA works with the bag of words (BoW) representation of a corpus, the goal of preprocessing is to construct a document-term matrix, which is a matrix that contains the count\n\nraw_data = tibble(read.csv(\"data/dataset.csv\"))\nraw_data[10,]$TITLE\n\n[1] \"$\\\\mathcal{R}_{0}$ fails to predict the outbreak potential in the presence of natural-boosting immunity\"\n\nraw_data[10,]$ABSTRACT\n\n[1] \"  Time varying susceptibility of host at individual level due to waning and\\nboosting immunity is known to induce rich long-term behavior of disease\\ntransmission dynamics. Meanwhile, the impact of the time varying heterogeneity\\nof host susceptibility on the shot-term behavior of epidemics is not\\nwell-studied, even though the large amount of the available epidemiological\\ndata are the short-term epidemics. Here we constructed a parsimonious\\nmathematical model describing the short-term transmission dynamics taking into\\naccount natural-boosting immunity by reinfection, and obtained the explicit\\nsolution for our model. We found that our system show \\\"the delayed epidemic\\\",\\nthe epidemic takes off after negative slope of the epidemic curve at the\\ninitial phase of epidemic, in addition to the common classification in the\\nstandard SIR model, i.e., \\\"no epidemic\\\" as $\\\\mathcal{R}_{0}\\\\leq1$ or normal\\nepidemic as $\\\\mathcal{R}_{0}&gt;1$. Employing the explicit solution we derived the\\ncondition for each classification.\\n\"\n\n\n\n\n\n\n\n\nDeletion with Caution\n\n\n\nBy convention, all data entries should be used in the analysis because the use of partial data might introduce bias in the analysis.\n\n\nfive different functions tidyr - unite, mutate, select dplyr - case_when https://www.kaggle.com/datasets/blessondensil294/topic-modeling-for-research-articles/\n\nraw_data = tibble(read.csv(\"data/dataset.csv\"))\nraw_data = raw_data[-16395,]\nraw_data = raw_data %&gt;%\n  unite(col=\"text\", TITLE:ABSTRACT, sep=\" \") %&gt;%\n  mutate(label=case_when(\n    Computer.Science==1 ~ 1,\n    Physics == 1 ~ 2,\n    Mathematics == 1 ~ 3,\n    Statistics == 1 ~ 4,\n    Quantitative.Biology == 1 ~ 5,\n    Quantitative.Finance == 1 ~ 6\n  )) %&gt;%\n  select(ID, text, label) %&gt;%\n  arrange(ID)\n\nraw_data$text = gsub(pattern = \"\\n\", replacement = \" \", x = raw_data$text)\nraw_data$text = gsub(pattern='\\\\b\\\\w{1,2}\\\\b', '', x = raw_data$text)\n\ntokens &lt;- raw_data$text %&gt;%\n          tokens(what = \"word\",\n                 remove_punct = TRUE,\n                 remove_symbols = TRUE,\n                 remove_numbers = TRUE,\n                 remove_url = TRUE,\n                 remove_separators = TRUE,\n                 split_hyphens= TRUE) %&gt;%\n          tokens_tolower() %&gt;%\n          tokens_remove(stopwords(\"english\")) %&gt;%\n          tokens_wordstem()\n\ndfm.final &lt;- dfm_trim(dfm(tokens), \n                # min_docfreq = 0.0001,\n                max_docfreq = 0.04, \n                docfreq_type = \"prop\",\n                verbose = TRUE) \ndtm = convert(dfm.final, to = \"tm\")\nlda_model = LDA(dtm, k=6, method=\"VEM\", control=list(seed=4))\ntopics.prob = posterior(lda_model, dtm)$topics\npred = apply(topics.prob, 1, which.max)\nlabel = raw_data$label\ntable(label, pred)\n\npred.df = tibble(label=label, final.pred=0, raw.pred=pred) %&gt;%\n  mutate(final.pred=case_when (\n    raw.pred == 1 ~ 1,\n    raw.pred == 4 ~ 2,\n    raw.pred == 2 ~ 3,\n    raw.pred == 5 ~ 4,\n    raw.pred == 3 ~ 5,\n    raw.pred == 6 ~ 6\n  )) \nresult_df = raw_data\nresult_df$pred = pred.df$final.pred\n  \npred.df= pred.df %&gt;%\n  group_by(label) %&gt;%\n  summarize(accuracy=mean(final.pred==label), count=n(), prop = count/nrow(raw_data))\nsum(pred.df$prop * pred.df$accuracy)\n\n\ntidy(lda_model, matrix=\"gamma\")\ntidy(lda_model, matrix=\"beta\")\n\nap_lda = lda_model\nap_topics &lt;- tidy(ap_lda, matrix = \"beta\")\nap_top_terms &lt;- ap_topics %&gt;%\n  group_by(topic) %&gt;%\n  slice_max(beta, n = 10) %&gt;% \n  ungroup() %&gt;%\n  arrange(topic, -beta)\n\nap_top_terms %&gt;%\n  mutate(term = reorder_within(term, beta, topic)) %&gt;%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_y_reordered()"
  },
  {
    "objectID": "analysis.html#step-1",
    "href": "analysis.html#step-1",
    "title": "Example analysis",
    "section": "Step 1",
    "text": "Step 1\nFirst, I deleted an incomplete document that contains only a few words common to many documents. Such a document is not informative about the topics it belong to and can pose problems in model training.\n\n# an incomplete document (title)\ncat(sprintf(raw_data[16395,]$TITLE))\n\nAre theoretical results 'Results'?\n\n# an incomplete document (abstract)\ncat(sprintf(raw_data[16395,]$ABSTRACT))\n\n  Yes.\n\nraw_data = raw_data[-16395,] # delete this incomplete document\n\n\n\n\n\n\n\nDeletion with Caution\n\n\n\nBy convention, all data entries should be used in the analysis because the use of partial data might introduce bias in the analysis. Any deletion of data entries needs to be well-justified."
  },
  {
    "objectID": "analysis.html#step-2",
    "href": "analysis.html#step-2",
    "title": "Example analysis",
    "section": "Step 2",
    "text": "Step 2\nSecond, I constructed the documents by combining the title and abstract for each paper. The topic labels are also adjusted to fit into one column.\n\n\nShow the code\nraw_data = raw_data %&gt;%\n  unite(col=\"text\", TITLE:ABSTRACT, sep=\" \") %&gt;% # combine title and abstract\n  mutate(label=case_when( # adjust topics labels\n    Computer.Science==1 ~ 1,\n    Physics == 1 ~ 2,\n    Mathematics == 1 ~ 3,\n    Statistics == 1 ~ 4,\n    Quantitative.Biology == 1 ~ 5,\n    Quantitative.Finance == 1 ~ 6\n  )) %&gt;%\n  select(ID, text, label) %&gt;% # retain only necessary information\n  arrange(ID)\n\n\n\n# an example data entry\nraw_data[10,]\n\n# A tibble: 1 × 3\n     ID text                                                               label\n  &lt;int&gt; &lt;chr&gt;                                                              &lt;dbl&gt;\n1    10 \"$\\\\mathcal{R}_{0}$ fails to predict the outbreak potential in th…     5"
  },
  {
    "objectID": "analysis.html#step-3",
    "href": "analysis.html#step-3",
    "title": "Example analysis",
    "section": "Step 3",
    "text": "Step 3\nThird, the texts are transformed to construct the document-term matrix. To be specific, new line characters, punctuations, non-word symbols, numbers, urls, separators, and words of length 1 and 2 are removed. Words are reduced to their stem form. For example, ‘received’, ‘receives’, and ‘receiving’ are all replaced by the root ‘receiv’. Moreover, common words that appear in more than 4% of the documents are removed. The remaining words make up the vocabulary, and the corresponding document-term matrix is constructed.\n\n\n\n\n\n\nStemming vs Lemmatization\n\n\n\n\n\nBoth stemming and lemmatization aim to reduce the inflected words into their base form, which can be viewed as a normalization of the words. A stemmer usually truncates the last few letters of a word based on simple rules. It runs faster but may generate a non-word pseudo stem. On the other hand, a lemmatizer considers both the morphology and the syntactic context of a word. Although it runs much slower and may deals with fewer cases, the result is more meaningful and human-friendly. For example, a lemmatizer can reduce ‘better’ to ‘good’ but a stemmer cannot.\n\n\n\n\n\nShow the code\n# remove new line characters\nraw_data$text = gsub(pattern = \"\\n\", replacement = \" \", x = raw_data$text)\n# remove words that have length 1 or 2\nraw_data$text = gsub(pattern='\\\\b\\\\w{1,2}\\\\b', '', x = raw_data$text)\n\ntokens &lt;- raw_data$text %&gt;%\n  tokens(what = \"word\",\n         remove_punct = TRUE,\n         remove_symbols = TRUE,\n         remove_numbers = TRUE,\n         remove_url = TRUE,\n         remove_separators = TRUE,\n         split_hyphens= TRUE) %&gt;% \n  tokens_tolower() %&gt;% \n  tokens_remove(stopwords(\"english\")) %&gt;%\n  tokens_wordstem()\n\ndfm.final &lt;- dfm_trim(dfm(tokens),\n                      max_docfreq = 0.04, \n                      docfreq_type = \"prop\",\n                      verbose = FALSE) \n# the final document-term matrix\ndtm = convert(dfm.final, to = \"tm\")\n\n\n\n# an tokenized example after stemming\ntokens[[\"text10\"]]\n\n  [1] \"mathcal\"     \"fail\"        \"predict\"     \"outbreak\"    \"potenti\"    \n  [6] \"presenc\"     \"natur\"       \"boost\"       \"immun\"       \"time\"       \n [11] \"vari\"        \"suscept\"     \"host\"        \"individu\"    \"level\"      \n [16] \"due\"         \"wane\"        \"boost\"       \"immun\"       \"known\"      \n [21] \"induc\"       \"rich\"        \"long\"        \"term\"        \"behavior\"   \n [26] \"diseas\"      \"transmiss\"   \"dynam\"       \"meanwhil\"    \"impact\"     \n [31] \"time\"        \"vari\"        \"heterogen\"   \"host\"        \"suscept\"    \n [36] \"shot\"        \"term\"        \"behavior\"    \"epidem\"      \"well\"       \n [41] \"studi\"       \"even\"        \"though\"      \"larg\"        \"amount\"     \n [46] \"avail\"       \"epidemiolog\" \"data\"        \"short\"       \"term\"       \n [51] \"epidem\"      \"construct\"   \"parsimoni\"   \"mathemat\"    \"model\"      \n [56] \"describ\"     \"short\"       \"term\"        \"transmiss\"   \"dynam\"      \n [61] \"take\"        \"account\"     \"natur\"       \"boost\"       \"immun\"      \n [66] \"reinfect\"    \"obtain\"      \"explicit\"    \"solut\"       \"model\"      \n [71] \"found\"       \"system\"      \"show\"        \"delay\"       \"epidem\"     \n [76] \"epidem\"      \"take\"        \"negat\"       \"slope\"       \"epidem\"     \n [81] \"curv\"        \"initi\"       \"phase\"       \"epidem\"      \"addit\"      \n [86] \"common\"      \"classif\"     \"standard\"    \"sir\"         \"model\"      \n [91] \"epidem\"      \"mathcal\"     \"leq1\"        \"normal\"      \"epidem\"     \n [96] \"mathcal\"     \"employ\"      \"explicit\"    \"solut\"       \"deriv\"      \n[101] \"condit\"      \"classif\""
  },
  {
    "objectID": "analysis.html#top-words",
    "href": "analysis.html#top-words",
    "title": "Example analysis",
    "section": "Top words",
    "text": "Top words\nLet’s inspect what are the key words selected for each topic. The following two figures show that LDA does a good job in recognizing most of the topics, except for quantitative biology. For example, the word that occurs with the highest probability for computer science is code. Moreover, other probable words robot, language, and visual may represents subfields robotics, NLP, and computer vision. Similar trends hold for other topics. However, the words selected for quantitative biology is completely unrelated to the topic. LDA seems to separate physics into general physics and astronomy, and the label that is matched to quantitative biology actually corresponds to astronomy.\n\n\nShow the code\nplot.df= pred.df %&gt;%\n  group_by(label) %&gt;%\n  summarize(accuracy=mean(final.pred==label), count=n(), prop = count/nrow(raw_data)) \nplot.df = plot.df %&gt;% \n  add_row(tibble_row(label=7, \n                     accuracy=sum(plot.df$accuracy*plot.df$prop), \n                     count=sum(plot.df$count), \n                     prop=1)\n  ) %&gt;%\n  arrange(label)\nplot.df$label = as.factor(plot.df$label)\nplot.df$topic = c(\"Computer Science\", \"Physics\", \"Mathematics\", \"Statistics\", \n                  \"Quantitative Biology\", \"Quantitative Finance\", \"Overall\")\n\ntop_terms = tidy(lda_model, matrix = \"beta\") %&gt;%\n  group_by(topic) %&gt;%\n  slice_max(beta, n = 10) %&gt;% # get the top 10 words within each topic\n  ungroup() %&gt;%\n  mutate(topic.final=case_when ( # match the predicted label with the true label\n    topic == 1 ~ 1,\n    topic == 4 ~ 2,\n    topic == 2 ~ 3,\n    topic == 5 ~ 4,\n    topic == 3 ~ 5,\n    topic == 6 ~ 6\n  )) %&gt;%\n  select(-topic) %&gt;%\n  rename(topic=topic.final) %&gt;%\n  arrange(topic, -beta)\n\nggplot(data=top_terms, aes(x=beta, y=reorder_within(x=term, by=beta, within=topic), \n                           fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~topic, scales=\"free\", \n             labeller=labeller(topic=c(\"1\"=plot.df$topic[1],\n                                       \"2\"=plot.df$topic[2],\n                                       \"3\"=plot.df$topic[3],\n                                       \"4\"=plot.df$topic[4],\n                                       \"5\"=plot.df$topic[5],\n                                       \"6\"=plot.df$topic[6]))) +\n  scale_y_reordered() +\n  labs(title=\"Top 10 Words by Topics\",\n       subtitle=\"The top 10 most probable words in each topic\",\n       caption=\"Data Source: Topic Modeling for Research Articles from Kaggle\",\n       x=\"Probability of Occurrence in Each Topic\",\n       y=\"Word\") +\n  theme(strip.text = element_text(size=12),\n        plot.title = element_text(size=14)) +\n  theme_minimal()\n\n\n\n\n\nFigure 1: The top 10 most probable words in each topic\n\n\n\n\n\n\nShow the code\ntop_terms = tidy(lda_model, matrix = \"beta\") %&gt;%\n  group_by(topic) %&gt;%\n  slice_max(beta, n = 30) %&gt;% # get the top 10 words within each topic\n  ungroup() %&gt;%\n  mutate(topic.final=case_when ( # match the predicted label with the true label\n    topic == 1 ~ 1,\n    topic == 4 ~ 2,\n    topic == 2 ~ 3,\n    topic == 5 ~ 4,\n    topic == 3 ~ 5,\n    topic == 6 ~ 6\n  )) %&gt;%\n  select(-topic) %&gt;%\n  rename(topic=topic.final) %&gt;%\n  arrange(topic, -beta)\n\nggplot(top_terms, aes(label=term, size=beta, color=beta)) +\n  geom_text_wordcloud() +\n  facet_wrap(~topic, scales=\"free\", \n             labeller=labeller(topic=c(\"1\"=plot.df$topic[1],\n                                       \"2\"=plot.df$topic[2],\n                                       \"3\"=plot.df$topic[3],\n                                       \"4\"=plot.df$topic[4],\n                                       \"5\"=plot.df$topic[5],\n                                       \"6\"=plot.df$topic[6])))  +\n  labs(title=\"Top 30 Words by Topics\",\n       subtitle=\"The top 30 most probable words in each topic\",\n       caption=\"Data Source: Topic Modeling for Research Articles from Kaggle\") +\n  theme(strip.text = element_text(size=12),\n        plot.title = element_text(size=14)) +\n  theme_minimal()\n\n\n\n\n\nFigure 2: Wordcloud of top 30 words in each topic"
  },
  {
    "objectID": "analysis.html#accuracy",
    "href": "analysis.html#accuracy",
    "title": "Example analysis",
    "section": "Accuracy",
    "text": "Accuracy\nNow, let’s see how accurate LDA does in categorizing research papers. LDA assigns each paper with a probability vector representing the proportion of each topic in the paper. For example, a paper assigned with [0.1, 0.1, 0.2, 0.05, 0.05, 0.3] is a mixture of 10% topic 1, 10% topic 2, 20% topic 3, 5% topic 4, 5% topic 5, and 30% topic 6. To ease the comparison, assume that each paper belongs to the topic that it has the highest proportion in. In this example, the paper will be assigned to topic 6. Following this scheme, LDA achieves an overall accuracy of 51%.\nThe overall performance differs from the model performance within each cluster. Figure 4 shows that there is a high variation among accuracy in different topics. Though it may be possible that the imbalanced labels shown in Figure 3 contribute to this, we can see that the effect of sample size on accuracy is not monotone. Several reasons might explain this. One is that the number of categories detected by LDA is different from our specified cluster number. The previous section shows that LDA separates the general physics and astronomy into two categories. Since both of them may be easier to detect than quantitative biology, LDA completely ignores the category of quantitative biology and results in a low accuracy. A larger input value for the number of clusters is needed for LDA. The other reason is that topics overlap with each other. Papers in an interdisciplinary field may be categorized as a even mixture of many topics. Thus, it is may not be reasonable to use a single topic to characterize them. Nevertheless, we see the strength of LDA that even categories with a small sample size can have a high clustering accuracy, as long as the top words in that category is correctly identified.\n\n\nShow the code\nggplot(data=plot.df[-nrow(plot.df),], aes(x=label, y=prop)) +\n  geom_bar(stat=\"identity\", width=0.8) +\n  geom_text(aes(label = round(prop, 2)), vjust = -0.5, size = 4) +\n  scale_x_discrete(labels = plot.df$topic) +\n  ylim(0, 0.5) + \n  labs(title=\"Distribution of Topics\",\n       subtitle=\"The proportion of research articles in each topic\",\n       caption=\"Data Source: Topic Modeling for Research Articles from Kaggle\",\n       x=\"Topic\",\n       y=\"Proportion\") + \n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1, size=10), \n        axis.title.x = element_text(size = 12), \n        axis.title.y = element_text(size = 12),\n        plot.title = element_text(size=14))\n\n\n\n\n\nFigure 3: The distribution of topics in the corpus\n\n\n\n\n\n\nShow the code\nggplot(data=plot.df, aes(x=label, y=accuracy)) +\n  geom_bar(stat=\"identity\", width=0.8) +\n  geom_text(aes(label = round(accuracy, 2)), vjust = -0.5, size = 4) +\n  scale_x_discrete(labels = plot.df$topic) +\n  ylim(0, 0.8) + \n  labs(title=\"LDA Clustering Accuracy\",\n       subtitle=\"The clustering accuracy of LDA by topics\",\n       caption=\"Data Source: Topic Modeling for Research Articles from Kaggle\",\n       x=\"Topic\",\n       y=\"Accuracy\") + \n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1, size=10), \n        axis.title.x = element_text(size = 12), \n        axis.title.y = element_text(size = 12),\n        plot.title = element_text(size=14))\n\n\n\n\n\nFigure 4: The clustering accuracy of LDA by topics"
  }
]