print(i)
}
}
for (i in 1:20972) {
if (length(tokens[[i]]) < 3) {
print(i)
}
}
dfm.final <- dfm_trim(dfm(tokens),
min_docfreq = 0.0005,
max_docfreq = 0.1,
docfreq_type = "prop",
min = 3,
verbose = TRUE)
dfm.final
rowSums(dfm.final)[1:10]
sum(rowSums(dfm.final)<=1)
dtm = convert(dfm.final, to = "tm")
lda_model = LDA(dtm, k=6, method="VEM", control=list(seed=2))
topics.prob = posterior(lda_model, dtm)$topics
pred = apply(topics.prob, 1, which.max)
label = raw_data$label
table(label, pred)
pred.df = tibble(label=label, final.pred=0, raw.pred=pred) %>%
mutate(final.pred=case_when (
raw.pred == 6 ~ 1,
raw.pred == 3 ~ 2,
raw.pred == 5 ~ 3,
raw.pred == 2 ~ 4,
raw.pred == 4 ~ 5,
raw.pred == 1 ~ 6
))
result_df = raw_data
result_df$pred = pred.df$final.pred
pred.df= pred.df %>%
group_by(label) %>%
summarize(accuracy=mean(final.pred==label), count=n(), prop = count/nrow(raw_data))
sum(pred.df$prop * pred.df$accuracy)
tidy(lda_model, matrix="gamma")
tidy(lda_model, matrix="beta")
ap_lda = lda_model
ap_topics <- tidy(ap_lda, matrix = "beta")
ap_top_terms <- ap_topics %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
arrange(topic, -beta)
ap_top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
scale_y_reordered()
dfm.final <- dfm_trim(dfm(tokens),
min_docfreq = 0.0001,
max_docfreq = 0.05,
docfreq_type = "prop",
min = 3,
verbose = TRUE)
dfm.final <- dfm_trim(dfm(tokens),
min_docfreq = 0.0001,
max_docfreq = 0.05,
docfreq_type = "prop",
verbose = TRUE)
dtm = convert(dfm.final, to = "tm")
lda_model = LDA(dtm, k=6, method="VEM", control=list(seed=2))
topics.prob = posterior(lda_model, dtm)$topics
pred = apply(topics.prob, 1, which.max)
label = raw_data$label
table(label, pred)
tidy(lda_model, matrix="gamma")
tidy(lda_model, matrix="beta")
ap_lda = lda_model
ap_topics <- tidy(ap_lda, matrix = "beta")
ap_top_terms <- ap_topics %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
arrange(topic, -beta)
ap_top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
scale_y_reordered()
dfm.final <- dfm_trim(dfm(tokens),
min_docfreq = 0.0001,
max_docfreq = 0.03,
docfreq_type = "prop",
verbose = TRUE)
dtm = convert(dfm.final, to = "tm")
lda_model = LDA(dtm, k=6, method="VEM", control=list(seed=1))
dfm.final <- dfm_trim(dfm(tokens),
min_docfreq = 0.0001,
max_docfreq = 0.05,
docfreq_type = "prop",
verbose = TRUE)
dtm = convert(dfm.final, to = "tm")
dtm = convert(dfm.final, to = "tm")
lda_model = LDA(dtm, k=6, method="VEM", control=list(seed=1))
topics.prob = posterior(lda_model, dtm)$topics
topics.prob = posterior(lda_model, dtm)$topics
pred = apply(topics.prob, 1, which.max)
label = raw_data$label
table(label, pred)
tidy(lda_model, matrix="gamma")
tidy(lda_model, matrix="beta")
ap_lda = lda_model
ap_topics <- tidy(ap_lda, matrix = "beta")
ap_top_terms <- ap_topics %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
arrange(topic, -beta)
ap_top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
scale_y_reordered()
raw_data = tibble(read.csv("data/train.csv"))
raw_data = raw_data %>%
unite(col="text", TITLE:ABSTRACT, sep=" ") %>%
mutate(label=case_when(
Computer.Science==1 ~ 1,
Physics == 1 ~ 2,
Mathematics == 1 ~ 3,
Statistics == 1 ~ 4,
Quantitative.Biology == 1 ~ 5,
Quantitative.Finance == 1 ~ 6
)) %>%
select(ID, text, label) %>%
arrange(ID)
raw_data$text = gsub(pattern='\\b\\w{1,2,3}\\b', '', x = raw_data$text)
raw_data$text = gsub(pattern='\\b\\w{1,2,3}\\b', '', x = raw_data$text)
raw_data$text = gsub(pattern='\\b\\w{1,3}\\b', '', x = raw_data$text)
raw_data$text = gsub(pattern = "\n", replacement = " ", x = raw_data$text)
tokens <- raw_data$text %>%
tokens(what = "word",
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_url = TRUE,
remove_separators = TRUE,
split_hyphens= TRUE) %>%
tokens_tolower() %>%
tokens_remove(stopwords("english")) %>%
tokens_wordstem()
dfm.final <- dfm_trim(dfm(tokens),
min_docfreq = 0.0001,
max_docfreq = 0.05,
docfreq_type = "prop",
verbose = TRUE)
dtm = convert(dfm.final, to = "tm")
lda_model = LDA(dtm, k=6, method="VEM", control=list(seed=1))
dfm.final
sum(rowSums(dfm.final)<=1)
sum(rowSums(dfm.final)<1)
dfm.final <- dfm_trim(dfm(tokens),
# min_docfreq = 0.0001,
max_docfreq = 0.05,
docfreq_type = "prop",
verbose = TRUE)
sum(rowSums(dfm.final)<1)
for (i in 1:20972) {
if (length(tokens[[i]]) < 3) {
print(i)
}
}
which(rowSums(dfm.final)<1)
tokens[[16395 ]]
raw_data$text[16395]
raw_data = tibble(read.csv("data/train.csv"))
raw_data = raw_data %>%
unite(col="text", TITLE:ABSTRACT, sep=" ") %>%
mutate(label=case_when(
Computer.Science==1 ~ 1,
Physics == 1 ~ 2,
Mathematics == 1 ~ 3,
Statistics == 1 ~ 4,
Quantitative.Biology == 1 ~ 5,
Quantitative.Finance == 1 ~ 6
)) %>%
select(ID, text, label) %>%
arrange(ID)
raw_data$text[16395]
raw_data = tibble(read.csv("data/train.csv"))
raw_data[16395,]
raw_data
raw_data = tibble(read.csv("data/train.csv"))
raw_data = raw_data[-16395,]
raw_data = raw_data %>%
unite(col="text", TITLE:ABSTRACT, sep=" ") %>%
mutate(label=case_when(
Computer.Science==1 ~ 1,
Physics == 1 ~ 2,
Mathematics == 1 ~ 3,
Statistics == 1 ~ 4,
Quantitative.Biology == 1 ~ 5,
Quantitative.Finance == 1 ~ 6
)) %>%
select(ID, text, label) %>%
arrange(ID)
raw_data$text = gsub(pattern='\\b\\w{1,3}\\b', '', x = raw_data$text)
raw_data$text = gsub(pattern='\\b\\w{1,3}\\b', '', x = raw_data$text)
raw_data$text = gsub(pattern = "\n", replacement = " ", x = raw_data$text)
raw_data$text = gsub(pattern = "\n", replacement = " ", x = raw_data$text)
tokens <- raw_data$text %>%
tokens(what = "word",
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_url = TRUE,
remove_separators = TRUE,
split_hyphens= TRUE) %>%
tokens_tolower() %>%
tokens_remove(stopwords("english")) %>%
tokens_wordstem()
dfm.final <- dfm_trim(dfm(tokens),
min_docfreq = 0.0001,
max_docfreq = 0.05,
docfreq_type = "prop",
verbose = TRUE)
which(rowSums(dfm.final)<1)
dfm.final <- dfm_trim(dfm(tokens),
min_docfreq = 0.0001,
max_docfreq = 0.03,
docfreq_type = "prop",
verbose = TRUE)
which(rowSums(dfm.final)<1)
raw_data = tibble(read.csv("data/train.csv"))
raw_data[c(17106, 18814),]
raw_data = tibble(read.csv("data/train.csv"))
raw_data = raw_data[-16395,]
raw_data = raw_data %>%
unite(col="text", TITLE:ABSTRACT, sep=" ") %>%
mutate(label=case_when(
Computer.Science==1 ~ 1,
Physics == 1 ~ 2,
Mathematics == 1 ~ 3,
Statistics == 1 ~ 4,
Quantitative.Biology == 1 ~ 5,
Quantitative.Finance == 1 ~ 6
)) %>%
select(ID, text, label) %>%
arrange(ID)
raw_data$text = gsub(pattern='\\b\\w{1,3}\\b', '', x = raw_data$text)
raw_data$text = gsub(pattern = "\n", replacement = " ", x = raw_data$text)
raw_data[c(17106, 18814),]
raw_data = tibble(read.csv("data/train.csv"))
raw_data[c(17107, 18815),]
raw_data = tibble(read.csv("data/train.csv"))
raw_data = raw_data[-16395,]
raw_data[c(17106, 18814),]
raw_data = raw_data %>%
unite(col="text", TITLE:ABSTRACT, sep=" ") %>%
mutate(label=case_when(
Computer.Science==1 ~ 1,
Physics == 1 ~ 2,
Mathematics == 1 ~ 3,
Statistics == 1 ~ 4,
Quantitative.Biology == 1 ~ 5,
Quantitative.Finance == 1 ~ 6
)) %>%
select(ID, text, label) %>%
arrange(ID)
raw_data$text = gsub(pattern = "\n", replacement = " ", x = raw_data$text)
raw_data$text = gsub(pattern='\\b\\w{1,3}\\b', '', x = raw_data$text)
raw_data[c(17106, 18814),]
raw_data = tibble(read.csv("data/train.csv"))
raw_data = raw_data[-16395,]
raw_data[c(17106, 18814),]
raw_data = tibble(read.csv("data/train.csv"))
raw_data = raw_data[-16395,]
raw_data = raw_data %>%
unite(col="text", TITLE:ABSTRACT, sep=" ") %>%
mutate(label=case_when(
Computer.Science==1 ~ 1,
Physics == 1 ~ 2,
Mathematics == 1 ~ 3,
Statistics == 1 ~ 4,
Quantitative.Biology == 1 ~ 5,
Quantitative.Finance == 1 ~ 6
)) %>%
select(ID, text, label) %>%
arrange(ID)
raw_data$text = gsub(pattern = "\n", replacement = " ", x = raw_data$text)
raw_data$text = gsub(pattern='\\b\\w{1,3}\\b', '', x = raw_data$text)
tokens <- raw_data$text %>%
tokens(what = "word",
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_url = TRUE,
remove_separators = TRUE,
split_hyphens= TRUE) %>%
tokens_tolower() %>%
tokens_remove(stopwords("english"))
dfm.final <- dfm_trim(dfm(tokens),
min_docfreq = 0.0001,
max_docfreq = 0.03,
docfreq_type = "prop",
verbose = TRUE)
dtm = convert(dfm.final, to = "tm")
lda_model = LDA(dtm, k=6, method="VEM", control=list(seed=1))
topics.prob = posterior(lda_model, dtm)$topics
pred = apply(topics.prob, 1, which.max)
label = raw_data$label
table(label, pred)
tidy(lda_model, matrix="gamma")
tidy(lda_model, matrix="beta")
ap_lda = lda_model
ap_topics <- tidy(ap_lda, matrix = "beta")
ap_top_terms <- ap_topics %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
arrange(topic, -beta)
ap_top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
scale_y_reordered()
raw_data = tibble(read.csv("data/train.csv"))
raw_data = raw_data[-16395,]
raw_data = raw_data %>%
unite(col="text", TITLE:ABSTRACT, sep=" ") %>%
mutate(label=case_when(
Computer.Science==1 ~ 1,
Physics == 1 ~ 2,
Mathematics == 1 ~ 3,
Statistics == 1 ~ 4,
Quantitative.Biology == 1 ~ 5,
Quantitative.Finance == 1 ~ 6
)) %>%
select(ID, text, label) %>%
arrange(ID)
raw_data$text = gsub(pattern = "\n", replacement = " ", x = raw_data$text)
raw_data$text = gsub(pattern='\\b\\w{1,2}\\b', '', x = raw_data$text)
tokens <- raw_data$text %>%
tokens(what = "word",
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_url = TRUE,
remove_separators = TRUE,
split_hyphens= TRUE) %>%
tokens_tolower() %>%
tokens_remove(stopwords("english")) %>%
tokens_wordstem()
dfm.final <- dfm_trim(dfm(tokens),
min_docfreq = 0.0001,
max_docfreq = 0.03,
docfreq_type = "prop",
verbose = TRUE)
dtm = convert(dfm.final, to = "tm")
lda_model = LDA(dtm, k=6, method="VEM", control=list(seed=1))
dfm.final <- dfm_trim(dfm(tokens),
min_docfreq = 0.0001,
max_docfreq = 0.05,
docfreq_type = "prop",
verbose = TRUE)
dtm = convert(dfm.final, to = "tm")
lda_model = LDA(dtm, k=6, method="VEM", control=list(seed=1))
lda_model = LDA(dtm, k=6, method="VEM", control=list(seed=1))
topics.prob = posterior(lda_model, dtm)$topics
pred = apply(topics.prob, 1, which.max)
label = raw_data$label
table(label, pred)
tidy(lda_model, matrix="gamma")
tidy(lda_model, matrix="beta")
ap_lda = lda_model
ap_topics <- tidy(ap_lda, matrix = "beta")
ap_top_terms <- ap_topics %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
arrange(topic, -beta)
ap_top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
scale_y_reordered()
lda_model = LDA(dtm, k=6, method="VEM", control=list(seed=10))
topics.prob = posterior(lda_model, dtm)$topics
pred = apply(topics.prob, 1, which.max)
label = raw_data$label
table(label, pred)
tidy(lda_model, matrix="gamma")
tidy(lda_model, matrix="beta")
ap_lda = lda_model
ap_topics <- tidy(ap_lda, matrix = "beta")
ap_top_terms <- ap_topics %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
arrange(topic, -beta)
ap_top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
scale_y_reordered()
dfm.final <- dfm_trim(dfm(tokens),
# min_docfreq = 0.0001,
max_docfreq = 0.04,
docfreq_type = "prop",
verbose = TRUE)
dtm = convert(dfm.final, to = "tm")
dtm = convert(dfm.final, to = "tm")
lda_model = LDA(dtm, k=6, method="VEM", control=list(seed=10)
raw_data = tibble(read.csv("data/train.csv"))
raw_data = tibble(read.csv("data/train.csv"))
raw_data = raw_data[-16395,]
raw_data = raw_data %>%
unite(col="text", TITLE:ABSTRACT, sep=" ") %>%
mutate(label=case_when(
Computer.Science==1 ~ 1,
Physics == 1 ~ 2,
Mathematics == 1 ~ 3,
Statistics == 1 ~ 4,
Quantitative.Biology == 1 ~ 5,
Quantitative.Finance == 1 ~ 6
)) %>%
select(ID, text, label) %>%
arrange(ID)
raw_data$text = gsub(pattern = "\n", replacement = " ", x = raw_data$text)
raw_data$text = gsub(pattern='\\b\\w{1,2}\\b', '', x = raw_data$text)
tokens <- raw_data$text %>%
tokens(what = "word",
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_url = TRUE,
remove_separators = TRUE,
split_hyphens= TRUE) %>%
tokens_tolower() %>%
tokens_remove(stopwords("english")) %>%
tokens_wordstem()
dfm.final <- dfm_trim(dfm(tokens),
# min_docfreq = 0.0001,
max_docfreq = 0.04,
docfreq_type = "prop",
verbose = TRUE)
dtm = convert(dfm.final, to = "tm")
lda_model = LDA(dtm, k=6, method="VEM", control=list(seed=4))
topics.prob = posterior(lda_model, dtm)$topics
pred = apply(topics.prob, 1, which.max)
label = raw_data$label
table(label, pred)
tidy(lda_model, matrix="gamma")
tidy(lda_model, matrix="beta")
ap_lda = lda_model
ap_topics <- tidy(ap_lda, matrix = "beta")
ap_top_terms <- ap_topics %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
arrange(topic, -beta)
ap_top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
scale_y_reordered()
pred.df = tibble(label=label, final.pred=0, raw.pred=pred) %>%
mutate(final.pred=case_when (
raw.pred == 1 ~ 1,
raw.pred == 4 ~ 2,
raw.pred == 2 ~ 3,
raw.pred == 5 ~ 4,
raw.pred == 3 ~ 5,
raw.pred == 6 ~ 6
))
result_df = raw_data
result_df$pred = pred.df$final.pred
pred.df= pred.df %>%
group_by(label) %>%
summarize(accuracy=mean(final.pred==label), count=n(), prop = count/nrow(raw_data))
sum(pred.df$prop * pred.df$accuracy)
?tibble
?gsub
?tokens
?dfm_trim
?convert
# Load the ggplot2 package if it's not already loaded
library(ggplot2)
# Create a sample data frame with a grouping variable
data <- data.frame(
Category = c("A", "B", "C", "D"),
Value = c(10, 15, 7, 12),
Group = c("Group 1", "Group 1", "Group 2", "Group 2"),
LineValue = c(0.5, 0.7, 0.3, 0.6)  # Additional values for the line plot
)
# Create a dual-axis plot with overlapping bar and line plots
gg <- ggplot(data, aes(x = Category)) +
geom_bar(aes(y = Value, fill = Group), stat = "identity", position = "dodge", width = 0.5) +
geom_text(aes(label = Value), vjust = -0.5, size = 4) +
labs(title = "Dual-Axis Overlapping Plot", x = "Categories") +
scale_fill_manual(values = c("Group 1" = "blue", "Group 2" = "red")) +
theme(axis.text.y = element_text(color = "blue"),  # Adjust the color of the left-side y-axis labels
axis.title.y = element_text(color = "blue"),  # Adjust the color of the left-side y-axis title
plot.title = element_text(size = 14))
# Create a line plot and a secondary y-axis
gg_line <- gg +
geom_line(aes(y = LineValue * max(data$Value)), color = "green") +
scale_y_continuous(sec.axis = sec_axis(~ . / max(data$Value), name = "Line Values")) +
theme(axis.text.y.right = element_text(color = "green"),  # Adjust the color of the right-side y-axis labels
axis.title.y.right = element_text(color = "green"))  # Adjust the color of the right-side y-axis title
gg_line
